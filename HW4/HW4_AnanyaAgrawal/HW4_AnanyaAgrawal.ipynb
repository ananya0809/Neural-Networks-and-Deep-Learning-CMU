{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, GATv2Conv, BatchNorm, global_add_pool, global_max_pool, global_mean_pool\n",
    "from torch_geometric.utils import degree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "train_data = torch.load(\"./data/train.pt\")\n",
    "test_data = torch.load(\"./data/test.pt\")\n",
    "\n",
    "def enrich_graph(data):\n",
    "    deg = degree(data.edge_index[0], num_nodes=data.num_nodes).view(-1, 1)\n",
    "    x = torch.cat([data.x, deg], dim=1)\n",
    "    num_nodes = x.size(0)\n",
    "    virtual_node = x.mean(dim=0, keepdim=True)\n",
    "    x = torch.cat([x, virtual_node], dim=0)\n",
    "    virtual_edges = torch.tensor([[num_nodes]*num_nodes, list(range(num_nodes))], dtype=torch.long)\n",
    "    edge_index = torch.cat([data.edge_index, virtual_edges, virtual_edges.flip(0)], dim=1)\n",
    "    data.x = x\n",
    "    data.edge_index = edge_index\n",
    "    return data\n",
    "\n",
    "train_data = [enrich_graph(data) for data in train_data]\n",
    "test_data = [enrich_graph(data) for data in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "all_data = train_data + test_data\n",
    "all_features = torch.cat([data.x for data in all_data], dim=0)\n",
    "scaler.fit(all_features)\n",
    "for data in all_data:\n",
    "    data.x = torch.tensor(scaler.transform(data.x), dtype=torch.float)\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        hidden_dim = 512\n",
    "        dropout = 0.45388\n",
    "        nn1 = torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        nn2 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        nn3 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm(hidden_dim)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm(hidden_dim)\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = BatchNorm(128)\n",
    "        self.lin1 = torch.nn.Linear(128 * 3, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.act = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.act(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        res = x\n",
    "        x = self.act(self.bn2(self.conv2(x, edge_index))) + res\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.conv3(x, edge_index)))\n",
    "        x_add = global_add_pool(x, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x = torch.cat([x_add, x_max, x_mean], dim=1)\n",
    "        x = self.act(self.lin1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.lin2(x).view(-1)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "input_dim = train_data[0].x.shape[1]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_preds = []\n",
    "epoch_logs = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    train_split = [train_data[i] for i in train_idx]\n",
    "    val_split = [train_data[i] for i in val_idx]\n",
    "    train_loader = DataLoader(train_split, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_split, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = GIN(input_dim).to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001133, weight_decay=1.66e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = F.l1_loss(out, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                loss = F.l1_loss(out, batch.y.view(-1))\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_model_fold{fold}.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= 10:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    epoch_logs.append((train_loss_history, val_loss_history))\n",
    "\n",
    "    with h5py.File(f\"model_fold{fold}.h5\", 'w') as hf:\n",
    "        for name, param in model.named_parameters():\n",
    "            hf.create_dataset(name, data=param.cpu().detach().numpy())\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"best_model_fold{fold}.pt\"))\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            fold_preds.append(out.cpu())\n",
    "    all_preds.append(torch.cat(fold_preds))\n",
    "\n",
    "for i, (train_hist, val_hist) in enumerate(epoch_logs):\n",
    "    plt.plot(train_hist, label=f\"Fold {i+1} Train\")\n",
    "    plt.plot(val_hist, label=f\"Fold {i+1} Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Fold\")\n",
    "plt.legend()\n",
    "plt.savefig(\"training_validation_loss.png\")\n",
    "plt.close()\n",
    "\n",
    "final_preds = torch.stack(all_preds).mean(dim=0).numpy()\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "sample_submission[\"labels\"] = final_preds\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission saved to submission.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
